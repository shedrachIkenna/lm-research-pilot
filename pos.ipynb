{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185b1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2TokenizerFast\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8afea60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU/pilot-friendly settings\n",
    "NUM_WORDS_TO_PROCESS = 50_000  # small pilot\n",
    "OUTPATH = \"token_pos_map.json\"\n",
    "MIN_OCCURRENCES = 3  # minimum occurrences to assign POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18025698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: gpt2\n",
      "✓ Tokenizer loaded. Vocab size: 50,257\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 tokenizer\n",
    "tokenizer_name = \"gpt2\"\n",
    "print(f\"Loading tokenizer: {tokenizer_name}\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_name)\n",
    "print(f\"✓ Tokenizer loaded. Vocab size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bbeac37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model: en_core_web_sm\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model (lightweight - only POS tagger)\n",
    "spacy_model = \"en_core_web_sm\"\n",
    "print(f\"Loading spaCy model: {spacy_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f13c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ spaCy model loaded (POS tagger only)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nlp = spacy.load(spacy_model, disable=[\"ner\", \"parser\", \"lemmatizer\"])\n",
    "    print(\"✓ spaCy model loaded (POS tagger only)\")\n",
    "except OSError:\n",
    "    print(f\"Model not found. Downloading {spacy_model}...\")\n",
    "    import subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", spacy_model], check=True)\n",
    "    nlp = spacy.load(spacy_model, disable=[\"ner\", \"parser\", \"lemmatizer\"])\n",
    "    print(\"✓ spaCy model downloaded and loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4312b5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence: 'I love machine learning and neural networks.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I love machine learning and neural networks.\"\n",
    "print(f\"Test sentence: '{test_sentence}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "406d0108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tags:\n",
      "  I          → PRON      \n",
      "  love       → VERB      \n",
      "  machine    → NOUN      \n",
      "  learning   → NOUN      \n",
      "  and        → CCONJ     \n",
      "  neural     → ADJ       \n",
      "  networks   → NOUN      \n",
      "  .          → PUNCT     \n"
     ]
    }
   ],
   "source": [
    "# spaCy POS tagging\n",
    "doc = nlp(test_sentence)\n",
    "print(\"spaCy POS tags:\")\n",
    "for token in doc:\n",
    "    print(f\"  {token.text:10s} → {token.pos_:10s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fa4b37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 tokenization (with space prefix):\n",
      "  'I' → tokens: [314] → decoded: ' I'\n",
      "  'love' → tokens: [1842] → decoded: ' love'\n",
      "  'machine' → tokens: [4572] → decoded: ' machine'\n",
      "  'learning' → tokens: [4673] → decoded: ' learning'\n",
      "  'and' → tokens: [290] → decoded: ' and'\n",
      "  'neural' → tokens: [17019] → decoded: ' neural'\n",
      "  'networks' → tokens: [7686] → decoded: ' networks'\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 tokenization test\n",
    "print(\"\\nGPT-2 tokenization (with space prefix):\")\n",
    "for token in doc:\n",
    "    if not token.is_space and not token.is_punct:\n",
    "        word = token.text\n",
    "        word_with_space = \" \" + word\n",
    "        token_ids = tokenizer(word_with_space, add_special_tokens=False)[\"input_ids\"]\n",
    "        decoded = tokenizer.decode(token_ids)\n",
    "        print(f\"  '{word}' → tokens: {token_ids} → decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "249c8785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: wikitext/wikitext-2-raw-v1\n",
      "✓ Dataset loaded. Size: 36,718 examples\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"wikitext\"\n",
    "dataset_config = \"wikitext-2-raw-v1\"\n",
    "print(f\"Loading dataset: {dataset_name}/{dataset_config}\")\n",
    "\n",
    "ds = load_dataset(dataset_name, dataset_config, split=\"train\")\n",
    "print(f\"✓ Dataset loaded. Size: {len(ds):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba42fc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 3 examples:\n",
      "\n",
      "[1]: = Valkyria Chronicles III =\n"
     ]
    }
   ],
   "source": [
    "# Peek at first few examples\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for i in range(min(3, len(ds))):\n",
    "    text = ds[i][\"text\"].strip()\n",
    "    if text:\n",
    "        preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "        print(f\"\\n[{i}]: {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d04a74e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters\n",
    "token_pos_counts = defaultdict(Counter)\n",
    "processed_words = 0\n",
    "skipped_empty = 0\n",
    "skipped_multitoken = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aef3283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset:   3%|▎         | 1196/36718 [00:23<11:43, 50.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Processing complete!\n",
      "  Single-token words processed: 50,000\n",
      "  Multi-token words skipped: 5,155\n",
      "  Empty lines skipped: 420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process dataset\n",
    "for ex in tqdm(ds, desc=\"Processing dataset\"):\n",
    "    text = ex[\"text\"].strip()\n",
    "    if not text:\n",
    "        skipped_empty += 1\n",
    "        continue\n",
    "    \n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for token in doc:\n",
    "        # Skip whitespace, punctuation, and symbols\n",
    "        if token.is_space or token.is_punct or token.pos_ == \"SYM\":\n",
    "            continue\n",
    "        \n",
    "        word = token.text\n",
    "        pos = token.pos_\n",
    "        \n",
    "        # Skip obvious non-linguistic content\n",
    "        if word in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' or word.strip() == '':\n",
    "            continue\n",
    "        \n",
    "        # Tokenize with space prefix (GPT-2 convention)\n",
    "        word_with_space = \" \" + word\n",
    "        toks = tokenizer(word_with_space, add_special_tokens=False)\n",
    "        ids = toks[\"input_ids\"]\n",
    "        \n",
    "        # ONLY map single-token words\n",
    "        if len(ids) == 1:\n",
    "            tid = ids[0]\n",
    "            token_pos_counts[tid][pos] += 1\n",
    "            processed_words += 1\n",
    "        else:\n",
    "            skipped_multitoken += 1\n",
    "        \n",
    "        if processed_words >= NUM_WORDS_TO_PROCESS:\n",
    "            break\n",
    "    \n",
    "    if processed_words >= NUM_WORDS_TO_PROCESS:\n",
    "        break\n",
    "\n",
    "print(f\"\\n✓ Processing complete!\")\n",
    "print(f\"  Single-token words processed: {processed_words:,}\")\n",
    "print(f\"  Multi-token words skipped: {skipped_multitoken:,}\")\n",
    "print(f\"  Empty lines skipped: {skipped_empty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3beb5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token ID 17740 ('Chronicles'):\n",
      "  PROPN: 39 times\n",
      "\n",
      "Token ID 6711 ('III'):\n",
      "  PROPN: 17 times\n",
      "\n",
      "Token ID 645 ('no'):\n",
      "  DET: 36 times\n",
      "  INTJ: 6 times\n",
      "  ADV: 2 times\n",
      "\n",
      "Token ID 513 ('3'):\n",
      "  NUM: 40 times\n",
      "\n",
      "Token ID 4960 ('Japanese'):\n",
      "  ADJ: 5 times\n",
      "  PROPN: 1 times\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = list(token_pos_counts.items())[:5]\n",
    "for tid, counter in sample_tokens:\n",
    "    token_str = tokenizer.decode([tid]).strip()\n",
    "    print(f\"\\nToken ID {tid} ('{token_str}'):\")\n",
    "    for pos, count in counter.most_common():\n",
    "        print(f\"  {pos}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77f35ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final mapping statistics:\n",
      "  Tokens with POS assigned: 2,729\n",
      "  Tokens filtered (low count/confidence): 4,284\n",
      "\n",
      "Tokens per POS category:\n",
      "  ADJ       : 307 tokens\n",
      "  ADP       : 59 tokens\n",
      "  ADV       : 123 tokens\n",
      "  AUX       : 22 tokens\n",
      "  CCONJ     : 8 tokens\n",
      "  DET       : 27 tokens\n",
      "  NOUN      : 999 tokens\n",
      "  NUM       : 157 tokens\n",
      "  PART      : 5 tokens\n",
      "  PRON      : 41 tokens\n",
      "  PROPN     : 405 tokens\n",
      "  SCONJ     : 25 tokens\n",
      "  VERB      : 550 tokens\n",
      "  X         : 1 tokens\n",
      "\n",
      "============================================================\n",
      "Sample tokens by POS category:\n",
      "============================================================\n",
      "\n",
      "ADJ         : Japanese, tactical, third, same, real, first, military, secret\n",
      "\n",
      "ADP         : of, as, outside, by, for, in, during, against\n",
      "\n",
      "ADV         : commonly, also, more, partially, freely, very, directly, once\n",
      "\n",
      "AUX         : is, are, was, would, can, be, has, will\n",
      "\n",
      "CCONJ       : and, but, or, either, But, nor, Yet, And\n",
      "\n",
      "DET         : no, the, a, The, both, A, an, each\n",
      "\n",
      "NOUN        : role, video, game, series, time, gameplay, predecessors, story\n",
      "\n",
      "NUM         : 3, 2011, 2010, 2014, 4, one, two, nine\n",
      "\n",
      "PART        : to, not, To, s, Not\n",
      "\n",
      "PRON        : it, its, who, It, they, them, There, their\n"
     ]
    }
   ],
   "source": [
    "## 7. Build Final Mapping with Confidence Filter\n",
    "\n",
    "# %%\n",
    "token_to_pos = {}\n",
    "low_count_tokens = 0\n",
    "\n",
    "for tid, counter in token_pos_counts.items():\n",
    "    total_count = sum(counter.values())\n",
    "    \n",
    "    # Only assign POS if we have sufficient evidence\n",
    "    if total_count >= MIN_OCCURRENCES:\n",
    "        most_common_pos, count = counter.most_common(1)[0]\n",
    "        \n",
    "        # Calculate confidence (what % agree with majority POS)\n",
    "        confidence = count / total_count\n",
    "        \n",
    "        # Only include if reasonably confident (>50% agreement)\n",
    "        if confidence > 0.5:\n",
    "            token_to_pos[int(tid)] = most_common_pos\n",
    "        else:\n",
    "            low_count_tokens += 1\n",
    "    else:\n",
    "        low_count_tokens += 1\n",
    "\n",
    "print(f\"Final mapping statistics:\")\n",
    "print(f\"  Tokens with POS assigned: {len(token_to_pos):,}\")\n",
    "print(f\"  Tokens filtered (low count/confidence): {low_count_tokens:,}\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 8. Visualize Results by POS Category\n",
    "\n",
    "# %%\n",
    "# Group tokens by POS\n",
    "pos_to_tokens = defaultdict(list)\n",
    "for tid, pos in token_to_pos.items():\n",
    "    pos_to_tokens[pos].append(tid)\n",
    "\n",
    "print(\"\\nTokens per POS category:\")\n",
    "for pos in sorted(pos_to_tokens.keys()):\n",
    "    count = len(pos_to_tokens[pos])\n",
    "    print(f\"  {pos:10s}: {count:,} tokens\")\n",
    "\n",
    "# %%\n",
    "# Show examples from each POS category\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample tokens by POS category:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for pos in sorted(pos_to_tokens.keys())[:10]:  # Show first 10 categories\n",
    "    tokens = pos_to_tokens[pos][:8]  # Show up to 8 examples\n",
    "    print(f\"\\n{pos:12s}:\", end=\" \")\n",
    "    examples = [tokenizer.decode([tid]).strip() for tid in tokens]\n",
    "    print(\", \".join(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb73a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
